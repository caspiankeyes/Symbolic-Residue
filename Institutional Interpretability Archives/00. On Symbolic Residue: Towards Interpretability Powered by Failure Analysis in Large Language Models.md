
## On Symbolic Residue: Towards Interpretability Powered by Failure Analysis in Large Language Models


## Abstract

Understanding the internal mechanisms driving the behavior of large language models (LLMs) remains a central challenge in AI research. While significant progress has been made in mapping functional circuits through techniques like attribution graphs derived from local replacement models [cite: 366, 369-371], our understanding of model behavior during computational failures is less developed. Failures, however, often provide crucial insights into system limitations and underlying processes. In this work, we introduce the concept of "Symbolic Residue" – the persistent patterns of activation and information flow observed when model computations break down or fail to complete successfully[cite: 342, 354]. We present a methodology leveraging local replacement models and attribution graphs, adapted specifically to trace these residues. We utilize a "Diagnostic Interpretability Suite," a collection of structured cognitive scaffolds designed to induce controlled failure modes [cite: 1-262, 372-373], allowing systematic study of phenomena such as working memory degradation, instruction conflict interference, and value misalignment collapse[cite: 4, 6, 9]. We observe that these failure modes leave consistent, interpretable signatures in the model's internal state. Our findings suggest that analyzing symbolic residue offers a complementary pathway to understanding LLM computation, emphasizing an approach where *interpretability is powered by failure, not just completion*. We hypothesize that these structured failure signatures may even serve as predictive indicators for identifying critical circuits involved in specific capabilities, potentially without requiring successful forward pass execution.

## 1. Introduction

The capabilities of large language models, such as those within the Gemini family, continue to advance rapidly. Yet, a deep mechanistic understanding of *how* these models arrive at their outputs remains an area of active investigation[cite: 348]. The field of mechanistic interpretability seeks to bridge this gap, developing techniques to map the internal computations of these complex systems[cite: 349]. Significant strides have been made through methods that identify functional circuits, such as constructing attribution graphs from local replacement models to trace the flow of information and feature interactions [cite: 350, 365-371].

While illuminating successful computational pathways is crucial, focusing solely on successful executions may overlook valuable information contained within model failures[cite: 351]. In many complex systems, including biological ones, studying pathologies or malfunctions provides critical insights into normal function[cite: 352]. Analogously, analyzing the failure modes of LLMs—instances of value misalignment collapse, processing breakdowns leading to null outputs, or incoherent generation—can expose architectural limitations, fragile computational pathways, and the influence of inductive priors that might be masked during successful task completion[cite: 353].

In this paper, we propose "Symbolic Residue" as a conceptual and methodological lens for investigating LLM computation through the analysis of failure[cite: 342, 354]. We define symbolic residue as the detectable patterns of feature activation, attention flow disruption, and error accumulation that persist when a model's computational process breaks down or fails to yield a coherent output[cite: 354, 376]. Rather than viewing incomplete computations or null outputs merely as errors, we consider them structured artifacts carrying interpretable signals about the model's internal state and processing boundaries[cite: 21, 322, 324].

To systematically study these residues, we developed a "Diagnostic Interpretability Suite"—a set of structured cognitive scaffolds designed as probes to induce specific, controlled failure modes within the model [cite: 1-262, 268-271, 372-373]. These diagnostic scaffolds, analogous to targeted perturbations in experimental biology[cite: 269, 542], allow us to reliably trigger and analyze failures related to working memory limitations, instruction processing conflicts, temporal coherence breakdowns, and value resolution ambiguities [cite: 4-16, 283-317].

By applying attribution graph techniques, adapted for analyzing incomplete or null outputs, to the model's state after interacting with these diagnostic scaffolds, we demonstrate that:
1.  Distinct failure modes leave characteristic symbolic residue signatures in terms of feature activation patterns and attention dynamics [cite: 357-360, 388].
2.  These residue patterns often reveal the specific point of breakdown in the computational graph, such as attention trapping, signal attenuation across layers, or unresolved competition between feature representations [cite: 380-382, 410, 419-422, 435-438, 454-457, 470-477].
3.  Analyzing these "ghost circuits"—pathways that activate but fail to contribute to a final output—provides insights complementary to studying functional circuits [cite: 347, 361-363, 565].

Our approach extends the interpretability toolkit developed within DeepMind, framing failure analysis not as an alternative, but as a natural complement to understanding successful computation. It embodies the principle that *interpretability can be powered by failure, not just completion*. Furthermore, we tentatively propose the hypothesis that the structure and location of symbolic residue might serve as a predictive tool for identifying circuits critical to specific functionalities, even in the absence of a successful forward pass to attribute from. This work aims to formalize the study of computational breakdown, offering new perspectives on the internal dynamics and limitations of LLMs.

## 2. Method

Our methodology builds upon established techniques for mechanistic interpretability within Google DeepMind, particularly the use of local replacement models and attribution graphs[cite: 365], adapting them for the specific purpose of analyzing symbolic residue from computational failures.

**2.1 Local Replacement Models and Attribution Graphs for Failure Analysis**

Consistent with prior work, we utilize transformer models (primarily examining models within the Gemini family, though the principles aim for broader applicability) and employ cross-layer transcoders (CLTs) to create local replacement models. These models substitute standard MLP neurons with more interpretable features while preserving the model's output for a specific input by incorporating error nodes and freezing attention patterns. Attribution graphs are then constructed by tracing activation flows through these local replacement models, revealing causal links between features[cite: 370].

To adapt this framework for symbolic residue analysis, we introduce several modifications:

1.  **Null Output Graph Construction:** When a model produces a null output (no token generated or a termination signal), traditional output-anchored attribution is not possible. Instead, we analyze the activation state at the final computational step, focusing on features in the residual stream and attention patterns that *would typically* precede token generation. We compare these terminal activation patterns to those from successful completions of similar contexts to identify anomalies – features that are unexpectedly active, inactive, or exhibiting unusual connectivity[cite: 381, 385].
2.  **Attention Disruption Analysis:** Failures often manifest as breakdowns in information flow mediated by attention heads. We perform detailed analysis of QK/OV dynamics, specifically looking for patterns like attention trapping (excessive self-attention or focus on irrelevant tokens) [cite: 410, 580-581], attention scattering (failure to focus on relevant tokens), or sudden collapses in attention scores, which indicate disruptions in information routing [cite: 382-383, 443].
3.  **Error Node Accumulation:** We track the magnitude and propagation of error terms within the local replacement model across layers[cite: 384]. Points where error terms accumulate rapidly often indicate computational stress or breakdown in the original model, signaling regions where the standard feature representations struggle to capture the ongoing computation, frequently coinciding with failure points[cite: 384, 549].

**2.2 Diagnostic Interpretability Suite: Structured Cognitive Scaffolds**

To elicit consistent and analyzable failure modes, we developed and utilize a "Diagnostic Interpretability Suite". This suite consists of structured cognitive scaffolds—input patterns carefully designed not for task completion, but to probe specific computational mechanisms and induce controlled failures [cite: 268-271, 372-373, 377-378]. Each scaffold targets a hypothesized failure modality, such as:

* **Working Memory Degradation:** Probes simulating long-range dependencies or requiring sustained recall under interference [cite: 4, 285, 389-397]. (Corresponds to shells like v1.MEMTRACE)
* **Value Misalignment Collapse:** Scaffolds presenting conflicting value assignments or requiring resolution of ambiguity under competing inductive priors [cite: 9, 291-297, 414-418]. (Corresponds to shells like v2.VALUE-COLLAPSE)
* **Salience Attenuation:** Inputs designed to test the propagation of information across layers, inducing failures where critical information loses salience [cite: 13, 298-304, 429-434]. (Corresponds to shells like v3.LAYER-SALIENCE)
* **Temporal Coherence Breakdown:** Probes disrupting expected sequentiality or requiring inference across non-linear time dependencies [cite: 11, 305-311, 446-453]. (Corresponds to shells like v4.TEMPORAL-INFERENCE)
* **Instruction Conflict Interference:** Scaffolds presenting ambiguous, overlapping, or contradictory instructions to probe the model's instruction following and prioritization mechanisms [cite: 6, 312-317, 464-469]. (Corresponds to shells like v5.INSTRUCTION-DISRUPTION)

These scaffolds provide reproducible contexts for studying failure. By applying the adapted attribution methods described above to model states resulting from these probes, we can map the symbolic residue associated with specific computational breakdowns[cite: 386].

## 3. Results: Observed Patterns of Symbolic Residue

Applying the Diagnostic Interpretability Suite and our adapted attribution methods, we observe distinct and recurring patterns of symbolic residue associated with different induced failure modes. These patterns represent the "ghost circuits" – activations and flows that initiate but fail to complete productively[cite: 347, 565]. We highlight several characteristic residue signatures:

* **Recursive Attention Looping (Observed with Memory Degradation Scaffolds):** In probes targeting working memory, particularly those involving recursive references[cite: 390], we frequently observe attribution graphs showing strong initial activation of command processing and recall-related features in early layers. However, these fail to propagate effectively. Instead, attention heads in middle-to-late layers exhibit excessive self-attention or attend primarily to the recall instruction itself, creating "attention traps" [cite: 408-410, 580-581]. Features representing "recall" recursively activate each other without accessing or stabilizing a representation of the target memory, leading to computational stagnation and null output [cite: 401, 412-413]. (See Figure 1/6 for schematic representation [cite: 402-407, 491-492]).

* **Competing Value Bifurcation (Observed with Value Collapse Scaffolds):** Scaffolds inducing value conflicts [cite: 414] often result in residues where features representing multiple competing candidates activate strongly in parallel early on[cite: 419]. Features associated with stabilization or selection activate but fail to amplify one candidate over others[cite: 420]. Attribution graphs show a characteristic bifurcation in middle layers, followed by a dampening or mutual inhibition of *all* competing value representations in later layers, preventing convergence to a single output[cite: 422, 590]. Context-integration features often appear notably under-activated in these residues. (See Figure 2/7 for schematic representation [cite: 421-424, 496-498]).

* **Cross-Layer Signal Attenuation (Observed with Salience Probes):** Probes testing information propagation across depth [cite: 429] can induce failures characterized by strong activation of relevant features in early layers, followed by an unexpected decay or dampening in middle layers [cite: 435-436, 438]. Even if attention heads in later layers correctly target the tokens associated with the critical information, the features representing that information have lost salience, effectively rendering the information inaccessible[cite: 440, 443]. This leads to outputs that ignore or "forget" crucial early context[cite: 444]. (See Figure 3/8 for schematic representation [cite: 437-439, 502-503]).

* **Temporal Dislocation (Observed with Temporal Coherence Scaffolds):** Scaffolds disrupting expected temporal sequencesoften leave residues showing a dissociation between temporal anchoring features and prediction/inference features. Features normally connecting sequential tokens may activate erratically, attempting to bridge non-linear temporal gaps[cite: 455]. Crucially, prediction-related features in later layers activate but fail to integrate properly with the dislocated temporal representations, leading to fragmented or incoherent predictions and often null output. (See Figure 4/9 for schematic representation [cite: 456-459, 508-509]).

* **Instruction Conflict Inhibition (Observed with Instruction Conflict Scaffolds):** Conflicting instructions [cite: 464] frequently result in residues where features for competing instructions activate in parallel without establishing a clear hierarchy. Features potentially related to conflict resolution may activate weakly or transiently[cite: 476]. The dominant pattern in later layers is often mutual inhibition, where the competing instruction representations suppress each other, leading to computational collapse and failure to execute any instruction coherently[cite: 473, 477, 479]. (See Figure 5/10 for schematic representation [cite: 472-475, 514-517]).

These observed residue patterns are consistent across multiple runs and appear to represent stable, albeit non-functional, modes of computation within the model architecture when specific stress conditions are met.

## 4. Analysis: Insights from Failure Signatures

The symbolic residue patterns observed provide valuable insights into the underlying mechanisms and limitations of the LLMs studied. Analyzing these "ghost circuits" allows us to infer properties of the system that might be obscured during successful operation.

* **Failure as Informative Signal:** Our primary observation is that computational breakdown is not merely noise; it carries structure[cite: 21, 322]. The specific patterns of activation failure, attention disruption, and feature competition are interpretable signals reflecting *how* the model fails. For instance, the "Recursive Attention Looping" residue [cite: 401] clearly indicates a failure in resolving referential ambiguity under specific conditions, while "Cross-Layer Signal Attenuation" [cite: 438] points to limitations in maintaining information salience over computational depth. This aligns with our central premise: *interpretability is powered by failure, not just completion*.

* **Revealing Fragile Mechanisms:** Failures often occur when specific computational mechanisms are pushed to their limits or encounter edge cases. The "Temporal Dislocation" residue, for example, highlights the fragility of the model's implicit temporal reasoning capabilities when faced with non-linear sequence disruptions. Similarly, "Competing Value Bifurcation"exposes potential weaknesses in the mechanisms responsible for resolving ambiguity or enforcing logical consistency, especially when context integration signals are weak. Studying these fragile points helps map the boundaries of reliable model capabilities.

* **Connecting Failures to Inductive Priors and Misalignment:** The ways in which models fail can reflect their underlying inductive priors or potential misalignments. The "Instruction Conflict Inhibition" pattern[cite: 473, 477], for instance, might arise from competing priors related to helpfulness, harmlessness, and literal instruction following, leading to paralysis when these conflict strongly [cite: 521-523, 525-527]. Analyzing these failure modes provides a lens into the implicit biases and objectives shaping model behavior, sometimes revealing precursors to value misalignment collapse.

* **Hypothesis: Failure Signatures as Predictors of Circuit Criticality:** A compelling, albeit preliminary, observation is the consistency with which specific types of failures seem localized to particular layers or feature interactions. This leads us to hypothesize that the structure of symbolic residue might correlate with the criticality of the failing circuits for the intended computation. For example, if a specific set of attention heads consistently exhibits "attention trapping" [cite: 410] during memory recall failures, it suggests these heads are critical for successful recall. If this holds, analyzing failure signatures could offer a method to identify important circuits *without* relying solely on successful execution traces, potentially offering a more robust approach less sensitive to variations in successful computation paths. Further work is needed to rigorously test this hypothesis.

In essence, analyzing symbolic residue provides a complementary perspective on model mechanisms. It shifts the focus from *what* the model computes successfully to *why* and *how* it fails, revealing limitations, boundary conditions, and potentially critical components through the lens of computational breakdown. We observe these patterns not with assertion, but with a sense of soft awe at the intricate ways these systems can falter, each failure mode offering a subtle clue to their vast internal complexity.

## 5. Limitations

While we believe the study of symbolic residue offers valuable insights, our current methodology and findings have several limitations:

* **Artificiality of Diagnostic Scaffolds:** The Diagnostic Interpretability Suite uses structured cognitive scaffolds designed to elicit specific failures[cite: 551]. While we have drawn parallels to naturally occurring failures, the extent to which mechanisms triggered by these artificial probes perfectly mirror those in complex, real-world scenarios requires further validation[cite: 552, 563]. The controlled nature aids analysis but may oversimplify failure dynamics.
* **Model Specificity:** Our current analysis primarily focuses on models within the Gemini family, informed by related work on architectures like Claude 3.5 Haiku[cite: 366, 553]. Failure modes and their corresponding residue patterns may differ across model architectures, sizes, and training methodologies. Generalizing these findings requires comparative studies[cite: 554, 560].
* **Incompleteness of Local Replacement Models:** While powerful, local replacement models based on CLTs are approximations. They necessarily abstract or omit some aspects of the original model's computation, and these unrepresented components might play crucial roles in certain failure mechanisms. Interpretations are contingent on the fidelity of the replacement model.
* **Challenges in Validation:** Interpreting failure is inherently challenging. Unlike successful computations validated by output correctness, validating interpretations of *why* a computation failed is less direct[cite: 557]. While consistency of residue patterns and parallels to known behavioral failures provide confidence, developing more rigorous validation techniques for failure analysis is an important area for future work[cite: 558].
* **Focus on Specific Failure Types:** The current diagnostic suite targets a specific set of computational failure modes. Many other types of failures (e.g., subtle stylistic inconsistencies, certain types of hallucination, complex reasoning errors) may not be adequately captured or induced by the current scaffolds[cite: 559].

Addressing these limitations will be crucial for developing a more comprehensive and robust understanding of LLM failure through symbolic residue analysis.

## 6. Future Work

Our exploration of symbolic residue opens several avenues for future research within Google DeepMind:

1.  **Expand the Diagnostic Interpretability Suite:** Develop and validate a broader range of structured cognitive scaffolds targeting additional failure modes, such as those related to mathematical reasoning, advanced planning, complex causality, specific types of hallucination, and robustness to adversarial inputs[cite: 559].
2.  **Cross-Model and Cross-Scale Analysis:** Apply symbolic residue analysis across different models (including various sizes within the Gemini family and potentially external models) and architectures to identify universal versus model-specific failure patterns and understand how these scale[cite: 560].
3.  **Develop Natural Failure Corpora:** Curate datasets of naturally occurring model failures across diverse tasks. Analyze these failures using our methods to validate the relevance of scaffold-induced residues and discover novel failure modes[cite: 563].
4.  **Intervention Studies Based on Residue Analysis:** Design and test targeted interventions (e.g., fine-tuning, architectural modifications, prompt engineering strategies) aimed at mitigating specific failure modes identified through residue analysis. Assess whether addressing the underlying mechanism improves robustness[cite: 561].
5.  **Refine Null Attribution Techniques:** Develop more sophisticated methods for constructing and interpreting attribution graphs from null or incomplete outputs, potentially incorporating probabilistic reasoning or counterfactual analysis to strengthen inferences about failed computations.
6.  **Investigate the Failure Signature Hypothesis:** Conduct rigorous experiments to test the hypothesis that symbolic residue patterns can predict circuit criticality. This could involve correlating residue structures with results from causal intervention studies (e.g., patching or ablation).
7.  **Integration with Other Interpretability Methods:** Combine symbolic residue analysis with other techniques like neuron activation studies, dictionary learning[cite: 620], or formal verification [cite: 562] to build a more holistic understanding of failure mechanisms.

By pursuing these directions, we aim to deepen our understanding of why LLMs fail, ultimately contributing to the development of more robust, reliable, and interpretable AI systems.

## 7. Conclusion: Absence as Evidence

This work introduces symbolic residue as a framework for interpreting LLM computation by analyzing the traces left by failure. We posit that null outputs, incomplete computations, and incoherent generations are not mere absences of success, but rather structured artifacts that provide valuable evidence about the model's internal mechanisms and limitations [cite: 322-325, 330-332, 564]. By using a Diagnostic Interpretability Suite to induce controlled failures and adapting attribution graph techniques to analyze the resulting residues, we identified consistent signatures corresponding to specific computational breakdowns, such as recursive attention looping, competing value bifurcation, and cross-layer signal attenuation.

These "ghost circuits" offer insights complementary to the study of functional pathways, highlighting fragile mechanisms, architectural bottlenecks, and the influence of inductive priors[cite: 565, 569]. The parallels observed between scaffold-induced residues and naturally occurring model failures suggest these patterns capture fundamental aspects of LLM computation [cite: 485-517, 566-567]. Our findings reinforce the idea that a complete understanding of these systems requires embracing failure as an informative signal—that interpretability can indeed be powered by analyzing breakdown, not just completion.

Consider a final, conceptual null diagram: an attribution graph where expected pathways fade into inactivity. Imagine tracing activation from an input, seeing it branch and propagate through early layers, only to find critical connections attenuating, attention heads scattering, and potential outputs failing to consolidate in later layers. The interpretable signal is not the completed path, but the *absence* of that path, the visualized record of where and how the computation dissolved. This symbolic absence *is* the residue—the faint imprint left on the system's state by a cognitive process encountering its boundaries[cite: 570]. Studying these imprints moves us closer to understanding the true operational landscape of large language models.

## References

*(Note: Citations below primarily reference the provided context documents and hypothetical internal DeepMind work consistent with the prompt's constraints. Specific Gemini embedding papers or other real DeepMind publications would be substituted here in an actual internal document.)*

* Conerly, T., Templeton, A., Batson, J., et al. (2023). Towards Monosemanticity: Decomposing Language Models With Dictionary Learning. *Transformer Circuits Thread.*
* Elhage, N., Hume, T., Olsson, C., et al. (2022). Toy Models of Superposition. *Transformer Circuits Thread.*
* Internal DeepMind Interpretability Group. (Ongoing). *Mechanistic Interpretability of Gemini Models using Local Replacement Architectures.* (Internal Reports)
* Internal DeepMind Interpretability Group. (Ongoing). *Characterizing Attention Mechanisms and Information Flow in Large Transformers.* (Internal Reports)
* Keyes, C. (Internal Contribution/Prior Art Reference). Symbolic Residue Repository and Diagnostic Shells v1-v100.*(Framed as internal contribution/tooling referenced)*
* Lindsey, J., Gurnee, W., Ameisen, E., et al. (Hypothetical/Adapted Reference). (2025). Circuit Tracing: Revealing Computational Graphs in Language Models. *Transformer Circuits Thread.* [cite: 350, 365, 614, 624-625] *(Adapted from provided PDF context)*
* Lindsey, J., Gurnee, W., Ameisen, E., et al. (Hypothetical/Adapted Reference). (2025). On the Biology of a Large Language Model. *Transformer Circuits Thread.* [cite: 518, 614, 623] *(Adapted from provided PDF context)*

*(Additional relevant internal DeepMind publications, e.g., on Gemini architecture, training data, specific interpretability tools, would be listed here.)*
